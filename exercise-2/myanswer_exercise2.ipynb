{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\NEELKANTH\n",
      "[nltk_data]     RAWAT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply filter 1: Check common non-stop-words\n",
    "def filter1(headline, first_sentence):\n",
    "    headline_words = set(headline.lower().split()) - stop_words\n",
    "    sentence_words = set(first_sentence.lower().split()) - stop_words\n",
    "    common_words = headline_words & sentence_words\n",
    "    return len(common_words) > 0\n",
    "\n",
    "# Function to apply filter 2: Check for bylines or extraneous marks\n",
    "def filter2(headline):\n",
    "    byline_pattern = re.compile(r'by\\s+\\w+', re.IGNORECASE)\n",
    "    extraneous_marks = ['â€”', '--']\n",
    "    if byline_pattern.search(headline):\n",
    "        return False\n",
    "    if any(mark in headline for mark in extraneous_marks):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Function to apply filter 3: Check for question marks or colons\n",
    "def filter3(headline):\n",
    "    return not (':' in headline or '?' in headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giving ID to each of the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ids_to_file(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        lines = infile.readlines()\n",
    "        for idx, line in enumerate(lines, start=1):\n",
    "            outfile.write(f\"{idx}\\t{line}\")\n",
    "\n",
    "# Specify the path to the input and output files\n",
    "input_file = 'alignedtest.txt'\n",
    "output_file = 'alignedtest_with_ids.txt'\n",
    "\n",
    "# Add IDs to the file\n",
    "add_ids_to_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_filters(test_file):\n",
    "    with open(test_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    total_pairs = len(lines)\n",
    "    filter1_pass = 0\n",
    "    filter2_pass = 0\n",
    "    filter3_pass = 0\n",
    "    all_filters_pass = 0\n",
    "\n",
    "    filter1_fail_ids = []\n",
    "    filter2_fail_ids = []\n",
    "    filter3_fail_ids = []\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split('\\t')\n",
    "        idx, first_sentence, headline = parts[0], parts[1], parts[2]\n",
    "        \n",
    "        pass_filter1 = filter1(headline, first_sentence)\n",
    "        pass_filter2 = filter2(headline)\n",
    "        pass_filter3 = filter3(headline)\n",
    "\n",
    "        if pass_filter1:\n",
    "            filter1_pass += 1\n",
    "        else:\n",
    "            filter1_fail_ids.append(idx)\n",
    "\n",
    "        if pass_filter2:\n",
    "            filter2_pass += 1\n",
    "        else:\n",
    "            filter2_fail_ids.append(idx)\n",
    "\n",
    "        if pass_filter3:\n",
    "            filter3_pass += 1\n",
    "        else:\n",
    "            filter3_fail_ids.append(idx)\n",
    "\n",
    "        if pass_filter1 and pass_filter2 and pass_filter3:\n",
    "            all_filters_pass += 1\n",
    "\n",
    "    print(f'Total pairs: {total_pairs}')\n",
    "    print(f'Pairs passing filter 1: {filter1_pass} ({(filter1_pass / total_pairs) * 100:.2f}%)')\n",
    "    print(f'Pairs passing filter 2: {filter2_pass} ({(filter2_pass / total_pairs) * 100:.2f}%)')\n",
    "    print(f'Pairs passing filter 3: {filter3_pass} ({(filter3_pass / total_pairs) * 100:.2f}%)')\n",
    "    print(f'Pairs passing all filters: {all_filters_pass} ({(all_filters_pass / total_pairs) * 100:.2f}%)')\n",
    "\n",
    "    return {\n",
    "        'filter1_fail_ids': filter1_fail_ids,\n",
    "        'filter2_fail_ids': filter2_fail_ids,\n",
    "        'filter3_fail_ids': filter3_fail_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 1951\n",
      "Pairs passing filter 1: 1645 (84.32%)\n",
      "Pairs passing filter 2: 1798 (92.16%)\n",
      "Pairs passing filter 3: 1602 (82.11%)\n",
      "Pairs passing all filters: 1246 (63.86%)\n",
      "IDs of a few lines failing filter 1: ['10', '44', '66', '82', '91']\n",
      "IDs of a few lines failing filter 2: ['6', '11', '23', '25', '27']\n",
      "IDs of a few lines failing filter 3: ['6', '41', '42', '43', '51']\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the test set file with IDs\n",
    "test_file = 'alignedtest_with_ids.txt'\n",
    "\n",
    "# Run the filter checks and get the IDs of lines that do not pass the filters\n",
    "failed_ids = check_filters(test_file)\n",
    "print(f\"IDs of a few lines failing filter 1: {failed_ids['filter1_fail_ids'][:5]}\")\n",
    "print(f\"IDs of a few lines failing filter 2: {failed_ids['filter2_fail_ids'][:5]}\")\n",
    "print(f\"IDs of a few lines failing filter 3: {failed_ids['filter3_fail_ids'][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 1951\n",
      "Pairs passing filter 1: 1645 (84.32%)\n",
      "Pairs passing filter 2: 1798 (92.16%)\n",
      "Pairs passing filter 3: 1602 (82.11%)\n",
      "Pairs passing all filters: 1246 (63.86%)\n",
      "\n",
      "Examples of statements failing filter 1:\n",
      "ID: 1880, Sentence: those who are behind the destabilization moves against the philippine government have no force to carry out their plan although the military establishment continues to receive intelligence reports about supposed destabilization plots , the military said monday ., Headline: results\\/standings of women 's field hockey champions trophy UNK UNK\n",
      "ID: 1277, Sentence: internal documents from bp show that there were serious problems and safety concerns with the deepwater horizon rig far earlier than those the company described to congress last week ., Headline: UNK UNK trims nyt\n",
      "ID: 1252, Sentence: the sloping expanse of verdant hillside known as el UNK does n't stand out amid the lush UNK straddling the remote UNK frontier ., Headline: hill of ore may hold mountain of conservation woes\n",
      "\n",
      "Examples of statements failing filter 2:\n",
      "ID: 6, Sentence: croatian president franjo tudjman said friday croatian and serb negotiators would meet saturday to thrash out an agreement on the last serb-held area in croatia , under a deal reached at us-brokered talks ., Headline: rebel serb talks to resume saturday : tudjman by peter UNK\n",
      "ID: 662, Sentence: the shape of india 's future government could be decided sunday as politicians huddled into meetings to work out alliances between parties and cobble together a coalition ., Headline: parties meet to work out a coalition to rule india by UNK roy\n",
      "ID: 85, Sentence: group of eight finance ministers warned saturday that the world economy was menaced by soaring energy costs and called for closer cooperation to calm volatile oil and gas markets ., Headline: g# ministers warn of energy risks seek cooperative action by nathaniel harrison UNK picture UNK UNK with final statement\n",
      "\n",
      "Examples of statements failing filter 3:\n",
      "ID: 1718, Sentence: UNK, Headline: #nd ld : pakistan 's musharraf steps down as army chief\n",
      "ID: 1377, Sentence: bill UNK , the power behind the riordan throne and a stein ally , said the political climate in los angeles is unlike any he has seen during his long years of watching city politics ., Headline: los angeles : for his challenge\n",
      "ID: 679, Sentence: this time , there were no gale conditions facing colin montgomerie at the oxfordshire golf club ., Headline: montgomerie weathers bad day advances to semifinals eds : UNK previous\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Load the test file to get statements\n",
    "def load_statements(test_file):\n",
    "    with open(test_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    statements = {}\n",
    "    for line in lines:\n",
    "        parts = line.strip().split('\\t')\n",
    "        idx, first_sentence, headline = parts[0], parts[1], parts[2]\n",
    "        statements[idx] = (first_sentence, headline)\n",
    "\n",
    "    return statements\n",
    "\n",
    "# Print random examples of statements failing each filter\n",
    "def print_filter_examples(statements, failed_ids, filter_num):\n",
    "    print(f\"\\nExamples of statements failing filter {filter_num}:\")\n",
    "    for _ in range(3):\n",
    "        idx = random.choice(failed_ids)\n",
    "        if idx in statements:\n",
    "            print(f\"ID: {idx}, Sentence: {statements[idx][0]}, Headline: {statements[idx][1]}\")\n",
    "        else:\n",
    "            print(f\"No statement found for ID: {idx}\")\n",
    "\n",
    "# Specify the path to the test set file with IDs\n",
    "test_file = 'alignedtest_with_ids.txt'\n",
    "\n",
    "# Run the filter checks and get the IDs of lines that do not pass the filters\n",
    "failed_ids = check_filters(test_file)\n",
    "\n",
    "# Load statements\n",
    "statements = load_statements(test_file)\n",
    "\n",
    "# Print examples for each filter\n",
    "print_filter_examples(statements, failed_ids['filter1_fail_ids'], 1)\n",
    "print_filter_examples(statements, failed_ids['filter2_fail_ids'], 2)\n",
    "print_filter_examples(statements, failed_ids['filter3_fail_ids'], 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer generated by the ChatGPT: The use of a mask token like UNK for rare words in headline generation can have both positive and negative impacts on the difficulty of the task, particularly in the context of automatic methods. Let's explore both perspectives:\n",
    "\n",
    "### Harder with Mask Token (UNK):\n",
    "1. **Semantic Loss**: Replacing rare words with a mask token can lead to a loss of semantic information. If important keywords or entities are masked out, it could hinder the generation of accurate and informative headlines.\n",
    "2. **ROUGE Metrics**: ROUGE metrics, commonly used for summarization evaluation, rely on the overlap of n-grams between generated summaries and reference summaries. If the mask token replaces crucial words that contribute to higher ROUGE scores, it could result in lower evaluation scores, indicating poorer performance.\n",
    "\n",
    "### Harder without Mask Token (Original Rare Words):\n",
    "1. **Sparsity Issues**: Rare words in headlines may lead to sparsity issues, especially during training of neural network models. Sparse representations could make it harder for the model to learn meaningful associations between words, potentially resulting in poorer headline generation.\n",
    "2. **Semantic Variability**: Rare words often have diverse meanings or interpretations, which could introduce ambiguity in headline generation. Models might struggle to capture the correct context or semantics of these rare words, leading to less coherent or relevant headlines.\n",
    "\n",
    "### Conclusion:\n",
    "In consideration of these points, it's arguable that while the use of a mask token like UNK introduces challenges, it might still be preferable to the alternative of retaining original rare words. The loss of semantic information due to masking could potentially be mitigated through effective modeling techniques such as attention mechanisms or improved data augmentation strategies. Additionally, the sparsity and semantic variability issues associated with original rare words might pose more significant obstacles to effective headline generation, particularly in the context of automatic methods. Therefore, while the presence of a mask token UNK may present its own set of challenges, it might ultimately offer a more tractable approach for headline generation, especially when evaluated using metrics like ROUGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
