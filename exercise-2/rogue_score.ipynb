{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install rouge-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.2727272727272727,\n",
       "  'p': 0.2727272727272727,\n",
       "  'f': 0.2727272727272727},\n",
       " 'rouge-2': {'r': 0.1, 'p': 0.1, 'f': 0.10000000000000002},\n",
       " 'rouge-4': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       " 'rouge-l': {'r': 0.2727272727272727,\n",
       "  'p': 0.2727272727272727,\n",
       "  'f': 0.2727272727272727},\n",
       " 'rouge-w-1.2': {'r': 0.15210311608148122,\n",
       "  'p': 0.24570650158950905,\n",
       "  'f': 0.18789251377101182},\n",
       " 'rouge-s4': {'r': 0.1, 'p': 0.1, 'f': 0.10000000000000002},\n",
       " 'rouge-su4': {'r': 0.14, 'p': 0.14, 'f': 0.14}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### I downloaded some different rogue code from the official implementation and it seems to be working\n",
    "### https://github.com/andersjo/pyrouge/tree/master/tools/ROUGE-1.5.5\n",
    "from rouge_metric import PyRouge\n",
    "\n",
    "### Load summary results\n",
    "# hypotheses = [\n",
    "#     'how are you\\ni am fine',  # document 1: hypothesis\n",
    "#     'it is fine today\\nwe won the football game',  # document 2: hypothesis\n",
    "# ]\n",
    "# references = [[\n",
    "#     'how do you do\\nfine thanks',  # document 1: reference 1\n",
    "#     'how old are you\\ni am three',  # document 1: reference 2\n",
    "# ], [\n",
    "#     'it is sunny today\\nlet us go for a walk',  # document 2: reference 1\n",
    "#     'it is a terrible day\\nwe lost the game',  # document 2: reference 2\n",
    "# ]]\n",
    "\n",
    "hypotheses = [\" Tokyo is the one of the biggest city in the world.\"]\n",
    "references = [[\"The capital of Japan, Tokyo, is the center of Japanese economy.\"]]\n",
    "\n",
    "\n",
    "# Evaluate document-wise ROUGE scores\n",
    "rouge = PyRouge(rouge_n=(1, 2, 4), rouge_l=True, rouge_w=True,\n",
    "                rouge_w_weight=1.2, rouge_s=True, rouge_su=True, skip_gap=4)\n",
    "scores = rouge.evaluate(hypotheses, references)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 (R-1):  0.23873000499352504\n",
      "ROUGE-2 (R-2):  0.08073371898649771\n",
      "ROUGE-L (R-L):  0.22251439787273927\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "def calculate_rouge_scores(hypothesis, references, n=1):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, references, avg=True)\n",
    "    return scores\n",
    "\n",
    "def print_rouge_scores(scores):\n",
    "    print(\"ROUGE-1 (R-1): \", scores['rouge-1']['f'])\n",
    "    print(\"ROUGE-2 (R-2): \", scores['rouge-2']['f'])\n",
    "    print(\"ROUGE-L (R-L): \", scores['rouge-l']['f'])\n",
    "\n",
    "# Read the contents of the files\n",
    "reference_file = \"D:\\\\Desktop\\\\master_scientific_computing\\\\second_semester\\\\problems with data\\\\Problems-With-Data--Summer-2024\\\\exercise-2\\\\references.txt\"\n",
    "with open(reference_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    reference_texts = f.readlines()\n",
    "\n",
    "schumann_summary_file = \"D:\\\\Desktop\\\\master_scientific_computing\\\\second_semester\\\\problems with data\\\\Problems-With-Data--Summer-2024\\\\exercise-2\\\\task4\\\\summaries_schumannetal_8.txt\"\n",
    "with open(schumann_summary_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    summary_texts = f.readlines()\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = calculate_rouge_scores(summary_texts, reference_texts)\n",
    "print_rouge_scores(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nec corp. and UNK said to join forces\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few things first. Let's compare Schumann's algorithm with text using NLI and get a small subset of faithful/unfaithful examples. We considered first 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the NLI model\n",
    "def load_model():\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model_name = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "    return tokenizer, model, device\n",
    "\n",
    "# Classify NLI\n",
    "def classify_nli(tokenizer, model, device, premise, hypothesis):\n",
    "    inputs = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.softmax(outputs.logits, dim=-1)[0]\n",
    "    entailment_score = prediction[0]\n",
    "    contradiction_score = prediction[2]\n",
    "    if entailment_score > contradiction_score:\n",
    "        predicted_label = \"entailment\"\n",
    "    else:\n",
    "        predicted_label = \"non-entailment\"\n",
    "    return predicted_label\n",
    "\n",
    "# Add numeric IDs to each line of the input files\n",
    "def add_numeric_ids(input_file, output_file):\n",
    "    with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "        lines = infile.readlines()\n",
    "        for idx, line in enumerate(lines, start=1):\n",
    "            outfile.write(f\"{idx}\\t{line.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying summaries: 1951it [08:08,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification completed.\n"
     ]
    }
   ],
   "source": [
    "# Process the generated summaries and classify them\n",
    "def classify_summaries(input_file, summaries_file, entailed_file, non_entailed_file):\n",
    "    tokenizer, model, device = load_model()\n",
    "    \n",
    "    entailed_ids = []\n",
    "    non_entailed_ids = []\n",
    "    \n",
    "    with open(input_file, \"r\") as infile, \\\n",
    "         open(summaries_file, \"r\") as sumfile, \\\n",
    "         open(entailed_file, \"w\") as entailed_outfile, \\\n",
    "         open(non_entailed_file, \"w\") as non_entailed_outfile:\n",
    "        \n",
    "        input_lines = infile.readlines()\n",
    "        summary_lines = sumfile.readlines()\n",
    "        \n",
    "        for input_line, summary_line in tqdm(zip(input_lines, summary_lines), desc=\"Classifying summaries\"):\n",
    "            input_id, input_text = input_line.strip().split(\"\\t\", 1)\n",
    "            summary_id, summary_text = summary_line.strip().split(\"\\t\", 1)\n",
    "            \n",
    "            if input_id != summary_id:\n",
    "                print(f\"IDs do not match for input ID {input_id} and summary ID {summary_id}\")\n",
    "                continue\n",
    "            \n",
    "            predicted_label = classify_nli(tokenizer, model, device, input_text, summary_text)\n",
    "            \n",
    "            if predicted_label == \"entailment\":\n",
    "                entailed_outfile.write(f\"{summary_text}\\n\")  # Write the summary text\n",
    "                entailed_ids.append(input_id)\n",
    "            else:\n",
    "                non_entailed_outfile.write(f\"{summary_text}\\n\")  # Write the summary text\n",
    "                non_entailed_ids.append(input_id)\n",
    "    \n",
    "    return entailed_ids, non_entailed_ids\n",
    "\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_file = \"input_with_id.txt\"\n",
    "    summaries_file = \"summaries_schumannetal_8_with_id.txt\"\n",
    "    \n",
    "    # Step 2: Classify summaries\n",
    "    entailed_file = \"entailed_summaries.txt\"\n",
    "    non_entailed_file = \"non_entailed_summaries.txt\"\n",
    "    \n",
    "    entailed_ids, non_entailed_ids = classify_summaries(input_file, summaries_file, entailed_file, non_entailed_file)\n",
    "    \n",
    "    # Step 3: Create reference files\n",
    "    with open(\"references_E.txt\", \"w\") as ref_e_file, open(\"references_with_id.txt\", \"r\") as ref_file:\n",
    "        ref_lines = ref_file.readlines()\n",
    "        for id in entailed_ids:\n",
    "            for line in ref_lines:\n",
    "                ref_id, ref_statement = line.strip().split(\"\\t\", 1)\n",
    "                if id == ref_id:\n",
    "                    ref_e_file.write(f\"{ref_statement}\\n\")\n",
    "                    break\n",
    "    \n",
    "    with open(\"references_NE.txt\", \"w\") as ref_ne_file, open(\"references_with_id.txt\", \"r\") as ref_file:\n",
    "        ref_lines = ref_file.readlines()\n",
    "        for id in non_entailed_ids:\n",
    "            for line in ref_lines:\n",
    "                ref_id, ref_statement = line.strip().split(\"\\t\", 1)\n",
    "                if id == ref_id:\n",
    "                    ref_ne_file.write(f\"{ref_statement}\\n\")\n",
    "                    break\n",
    "    \n",
    "    print(\"Classification completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the rouge score using faithful examples (labeled faithful by the NLI model) we found in the code-cell above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 (R-1):  0.2487153056897773\n",
      "ROUGE-2 (R-2):  0.0877460622278349\n",
      "ROUGE-L (R-L):  0.23179858449498694\n"
     ]
    }
   ],
   "source": [
    "# Read the contents of the files\n",
    "reference_file = \"references_E.txt\"\n",
    "with open(reference_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    reference_texts = f.readlines()\n",
    "\n",
    "schumann_summary_file = \"entailed_summaries.txt\"\n",
    "with open(schumann_summary_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    summary_texts = f.readlines()\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = calculate_rouge_scores(summary_texts, reference_texts)\n",
    "print_rouge_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the rogue score using the unfaithful examples (labeled so by the NLI model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 (R-1):  0.21202731763255223\n",
      "ROUGE-2 (R-2):  0.06198131333169726\n",
      "ROUGE-L (R-L):  0.19768662950439753\n"
     ]
    }
   ],
   "source": [
    "# Read the contents of the files\n",
    "reference_file = \"references_NE.txt\"\n",
    "with open(reference_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    reference_texts = f.readlines()\n",
    "\n",
    "schumann_summary_file = \"non_entailed_summaries.txt\"\n",
    "with open(schumann_summary_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    summary_texts = f.readlines()\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = calculate_rouge_scores(summary_texts, reference_texts)\n",
    "print_rouge_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the rouge scores increases when we only consider the subset of faithful examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
